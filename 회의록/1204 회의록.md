# 12/03 (화) 

## 진행 상황 정리
- 프로젝트에서 요구하는 바를 충실히 이행? -> 프로젝트 설명 pdf 조건 확인하면서 회의 진행
- hadoop 기반? -> 자연어 처리를 구현하고 있었으나 생각보다 어려워서 해보고 안되면 코랩으로. 나머지들은 hadoop 기반으로 구현할 것

## 데이터를 어떻게 분석할 것인가?
- 단어를 정책별 카테고리로 라벨링까지 했으나 기사를 정책별로 분류하는 방법이 필요   
-> 분석하기 위해서는 기사를 정책별로 분류하는 기준이 명확해야함

### 나온 방법
1. 기사 제목으로 TF-IDF 구해서 값을 더해 가장 값이 높은 단어의 카테고리를 정책으로 보기
2. 본문 내용으로 정책 단어를 count해서 가장 많이 나온 카테고리를 정책(제목으로만 보면 유의미한 결과가 안 나옴)으로 보기

### TF-IDF의 경우 지금까지 예상외의 결과가 많이 나와서 '2. 본문 내용 정책 단어 count'를 해봄
- 결과가 괜찮게 나옴. 정책이 아닌 기사를 잘 걸러내고, 정책별로 확실하게 나뉘어서 이걸로 진행하기로 결정   

- #### 월별 기사 정책 분류 -> 분석에 사용   
- #### 일별 기사 정책 분류 -> 분석 후 가격 변동의 원인 파악시 사용

----------------------------------------

## 다음 회의까지 할 일
- 윤철: 정책 단어 라벨링 수정, 시각화(부동산 데이터의 일정 기간 입력하면 해당하는 기간의 그래프만 보이게)와 분석
- 동준: 검색하면 gcp에서 찾아오는 api 같은 기능  (web 까진 아니더라도)
- 한결: 본문 내용으로 정책 단어 count 하는 코드 Hadoop으로 바꾸기
- 유빈: 자연처 처리(mecap) 조금 만 더 만져보기, '1. 기사 제목으로 TF-IDF 구해서 값을 더해기' 방법도 구현 해보기
 
----------------------------------------
## 다음 회의에서 할 것
- 분석 결과와 추가 분석 논의
- 보고서에 넣을 내용 정리
- 발표자 정하기
- 보고서 작성
- 발표 피피티 만들기

----------------------------------------
## 다음 회의
12/6 (금) 오전 11시 온라인

