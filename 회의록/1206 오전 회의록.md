# 12/06 오전 회의록

## 논의 사항

### 1. PySpark에서 잘 작동하지 않는 문제

- 정 안되면 일반 파일 시스템에서 토큰화된 문서를 HDFS에 업로드하여 진행

### 2. 시각화

- 서울 지역을 3가지로 분류
- 정책 1, 2, 3+4로 나누어 데이터에 매핑
- 지역별로 정책의 효과가 다르게 발생
- Baseline MSE, Linear Regression, Random Forest 세 가지 모델로 간단하게 ML 진행

### 3. 작업 분담

- 한결님이 담당하셨던 count 코드 Spark로 바꾸기 (태욱, 한결)
- 문자열 전체 비교
- 토큰화된 문서를 따로 만들어서 HDFS에 올리고 그것만 Spark로 다시 돌리기
- 정 안되면 HDFS에서 파일만 가져와서 로컬로 돌리기
- 월별, 기사별로 두 개 해야 함 (주 코드)
- 기사별 output에 ID, rank, month, policy, count는 있어야 함 (제목이나 본문은 빼도 되고, TF-IDF 대신 count. 빼도 됨)

### 4. 다른 문제

- 정책 3 + 정책 4 할 때, 먼저 합쳐서 돌려두면 문제 발생 (윤철)
  - 다른 코드 파일을 새로 만들어서 정책 3 + 정책 4 = 정책 3 하는 파일로 오버라이트

- count 선행되면 각 월별 정책 기사 n개 뽑는 코드(Spark) 구현해야 함 (동준)
  - 이거 선행되면 API로 기사 가져오는 건 바로 가능

- (x) 수정된 정책 단어로 제목 TF-IDF 점수 구한 score 문서 따로 만들어서 Spark 코드 돌려두기

### 5. 추가로 해야할 일

- 각자 작성한 코드 설명, output 설명 깃에 작성하기
- 보고서 (윤철님이 작성해오신 docs 기반으로 디벨롭)
- 큰 틀 정해놓고 구글 독스로 각자 맡은 부분 작성 
- ppt (해야함) 발표 역할분담.